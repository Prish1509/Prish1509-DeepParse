# Prish1509-DeepParse

Adaptive Log Interpretation Tool

What is a Log?
-A log is a timestamped record generated by software, systems, or devices to capture events, actions, or states during execution. Logs are widely used for:
  1.Debugging issues in software applications.
  2.Monitoring system performance and behavior.
  3.Security auditing and tracking unauthorized access.
  4.Analyzing user activity or operational events.

Example log entry:
2025-07-07 13:25:42 [INFO] Web server started on port 8000

-Logs usually contain structured or semi-structured text with fields like:
  1.Source/module name
  2.Log message
  3.Timestamp
  4.Severity level (INFO, WARNING, ERROR)


Methods Used in This Project
-This project uses three different ways to classify log data. Each method is good for a different kind of log:

1. Regular Expressions (Regex)
  -This method looks for simple and repeated patterns in logs.
  -It uses fixed rules to check if a log matches a known format.
2. Sentence Transformer with Logistic Regression
  -This method is used when we have enough training data.
  -It turns each log message into a numerical form (using Sentence Transformers) and then uses Logistic Regression to classify it.
3. Large Language Model (LLM)
  -This is helpful when we don’t have enough labeled data.
  -The model understands the text and tries to guess the right label based on its knowledge.

├── BERT_processor.py           # handles classification using sentence transformer + logistic regression
├── LLM_processor.py            # handles classification using LLM
├── classify.py                 # routes logs to the correct classification method
├── main.py                     # entry point to run the classification
├── regex_processor.py          # handles regex rule-based classification
├── packages.txt                # required python packages
├── .env                        # stores API keys (here for Groq)
├── models/
│   └── log_classifier.joblib   # saved logistic regression model
├── data/
│   ├── test.csv                # input CSV with log messages
├── Training/
│   ├── training.ipynb          # notebook used to train the ML model
│   └── dataset/
│       └── synthetic_logs.csv  # fake logs used for model training


How to Run:
1. Install Requirements
  Install the libraries you need:
  pip install -r packages.txt

2. Set Up Environment Variables
  Create a .env file and add your Groq API key like this:
  GROQ_API_KEY=your_groq_key_here

3. Run the Project
  Just run the main.py file:
  python main.py
  This will read the test.csv file from the data/ folder, classify all the log messages, and write the results to output.csv.

Notes
-Make sure log_classifier.joblib exists in the models/ folder. This model is trained using Training/training.ipynb.
-Input CSV must have two columns:
  source and log_message
-Output will have one more column: target_label



